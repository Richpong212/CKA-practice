# Kubernetes CKA Practice Questions and Solutions

---

## Q1. Create Storage Class, Persistent Volume, and Persistent Volume Claim

**SECTION: STORAGE**

**Cluster**: `cluster1-controlplane`

### Task:

- Create StorageClass `orange-stc-cka07-str`
- Create PV `orange-pv-cka07-str` (150Mi, ReadWriteOnce, Retain, local path `/opt/orange-data-cka07-str`, node affinity to `cluster1-controlplane`)
- Create PVC `orange-pvc-cka07-str` (128Mi, ReadWriteOnce)

### Solution:

SSH into the cluster1-controlplane host

```bash
ssh cluster1-controlplane
```

Create a yaml file as below:

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: orange-stc-cka07-str
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: orange-pv-cka07-str
spec:
  capacity:
    storage: 150Mi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: orange-stc-cka07-str
  local:
    path: /opt/orange-data-cka07-str
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - cluster1-controlplane

---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: orange-pvc-cka07-str
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: orange-stc-cka07-str
  volumeName: orange-pv-cka07-str
  resources:
    requests:
      storage: 128Mi
```

Apply the template:

```bash
kubectl apply -f <template-file-name>.yaml
```

---

## Q2. Find Node with Highest CPU Usage

**SECTION: CLUSTER ARCHITECTURE, INSTALLATION & CONFIGURATION**

### Solution:

Check CPU usage across all clusters:

```bash
ssh cluster1-controlplane "kubectl top node --no-headers | sort -nr -k2 | head -1"
ssh cluster3-controlplane "kubectl top node --no-headers | sort -nr -k2 | head -1"
ssh cluster4-controlplane "kubectl top node --no-headers | sort -nr -k2 | head -1"
```

Compare output:

- cluster1-controlplane: 583m
- cluster3-controlplane: 66m
- cluster4-controlplane: 145m

```bash
echo cluster1,cluster1-controlplane > /opt/high_cpu_node
```

---

## Q3. Create and Use PriorityClass

**SECTION: WORKLOADS & SCHEDULING**

**Cluster**: `cluster4-controlplane`

### Solution:

```bash
ssh cluster4-controlplane
```

Create the PriorityClass:

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class is for high-priority applications."
```

Apply it, then edit the deployment:

```bash
kubectl edit deployment hp-webapp -n high-priority
```

Inside `spec.template.spec`, add:

```yaml
priorityClassName: high-priority
```

Verify:

```bash
kubectl get deployment hp-webapp -n high-priority -o yaml | grep priorityClassName
```

---

## Q4. Upgrade Helm Chart

**SECTION: CLUSTER ARCHITECTURE, INSTALLATION & CONFIGURATION**

**Cluster**: `cluster1-controlplane`

### Solution:

```bash
ssh cluster1-controlplane
helm ls -n default
cd /root/
helm lint ./new-version
helm install --generate-name ./new-version --namespace default
helm uninstall webpage-server-01 -n default
```

---

## Q5. Fix Failed Deployment Under Resource Quota

**SECTION: WORKLOADS & SCHEDULING**

**Cluster**: `cluster3-controlplane`

### Solution:

```bash
ssh cluster3-controlplane
kubectl get deployment backend-api
kubectl describe pod <one-of-the-pods>
```

If you see errors like:

```
Error creating: pods ... is forbidden: exceeded quota: cpu-mem-quota, requested: requests.memory=128Mi, used: requests.memory=256Mi, limited: requests.memory=300Mi
```

You are **not allowed** to modify the resource limits or the quota.

Fix it by **reducing replicas**:

```bash
kubectl scale deployment backend-api --replicas=2
```

Then verify:

```bash
kubectl get pods
kubectl get deployment backend-api
```

---

## Q6. Deploy Vertical Pod Autoscaler (VPA)

**SECTION: WORKLOADS & SCHEDULING**

**Cluster**: `cluster1-controlplane`

### Solution:

```bash
ssh cluster1-controlplane
```

Apply VPA manifest:

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: cka24456
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Auto"
```

# Kubernetes CKA Practice Questions and Solutions

---

## Q1. Create Storage Class, Persistent Volume, and Persistent Volume Claim

... _(existing content unchanged)_

---

## Q7. Troubleshoot Ingress Resource

**SECTION: TROUBLESHOOTING**

**Cluster**: `cluster4-controlplane`

### Solution:

```bash
ssh cluster4-controlplane
kubectl create -f ingress.yaml
curl kodekloud-pink.app
kubectl edit svc pink-svc-cka16-trb
# Change protocol from UDP to TCP
curl kodekloud-pink.app
```

---

## Q8. Filter Pod Logs by ERROR

**SECTION: CLUSTER ARCHITECTURE, INSTALLATION & CONFIGURATION**

**Cluster**: `cluster1-controlplane`

### Solution:

```bash
ssh cluster1-controlplane
kubectl -n beta-cka01-arch logs beta-pod-cka01-arch | grep ERROR > /root/beta-pod-cka01-arch_errors
head /root/beta-pod-cka01-arch_errors
```

---

## Q9. Fix Service Accessibility

**SECTION: SERVICES AND NETWORKING**

**Cluster**: `cluster1-controlplane`

### Solution:

```bash
ssh cluster1-controlplane
kubectl exec curlpod-cka01-svcn -- curl curlme-cka01-svcn
kubectl delete svc curlme-cka01-svcn
kubectl expose pod curlme-cka01-svcn --port=80
```

---

## Q10. Fix Secret and Env in WebApp

**SECTION: WORKLOADS & SCHEDULING**

**Cluster**: `cluster3-controlplane`

### Solution:

```bash
ssh cluster3-controlplane
kubectl create secret generic db-secret-wl05 -n canara-wl05 \
--from-literal=DB_Host=mysql-svc-wl05 \
--from-literal=DB_User=root \
--from-literal=DB_Password=password123
```

Pod definition:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-pod-wl05
  namespace: canara-wl05
spec:
  containers:
    - name: webapp-pod-wl05
      image: kodekloud/simple-webapp-mysql
      envFrom:
        - secretRef:
            name: db-secret-wl05
```

```bash
kubectl replace -f <file> --force
curl http://<NODE-IP>:<NODE-PORT>
```

---

## Q11. Fix Mount Error in Deployment

**SECTION: TROUBLESHOOTING**

**Cluster**: `cluster4-controlplane`

### Solution:

```bash
ssh cluster4-controlplane
kubectl edit -n cka4974 deployments.apps nginx-frontend
```

Update volumeMounts:

```yaml
volumeMounts:
  - mountPath: /etc/nginx/conf.d/default.conf
    name: nginx-conf-vol
    subPath: default.conf
```

```bash
kubectl exec -it -n cka4974 deployments/nginx-frontend -- curl -I http://localhost:81
```

---

## Q12. Fix Ingress Host and Service

**SECTION: TROUBLESHOOTING**

**Cluster**: `cluster1-controlplane`

### Solution:

```bash
ssh cluster1-controlplane
kubectl edit ingress nodeapp-ing-cka08-trb
```

Update the following:

- `host:` to `kodekloud-ingress.app`
- `service.name:` to `nodeapp-svc-cka08-trb`
- `port.number:` to `3000`

```bash
curl http://kodekloud-ingress.app
```

---

## Q13. Add HTTPS Listener to Gateway

**SECTION: SERVICES AND NETWORKING**

**Cluster**: `cluster3-controlplane`

### Solution:

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: cka5673
spec:
  gatewayClassName: kodekloud
  listeners:
    - name: https
      protocol: HTTPS
      port: 443
      hostname: kodekloud.com
      tls:
        certificateRefs:
          - name: kodekloud-tls
```

---

## Q14. Route /api Traffic in HTTPRoute

**SECTION: SERVICES AND NETWORKING**

**Cluster**: `cluster3-controlplane`

### Solution:

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: web-route
  namespace: cka7395
spec:
  parentRefs:
    - name: nginx-gateway
      namespace: nginx-gateway
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /api
      backendRefs:
        - name: api-service
          port: 8080
    - matches:
        - path:
            type: PathPrefix
            value: /
      backendRefs:
        - name: web-service
          port: 80
```

---

## Q15. Create PVC for Existing PV

**SECTION: STORAGE**

**Cluster**: `cluster1-controlplane`

### Solution:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apple-pvc-cka04-str
spec:
  volumeName: apple-pv-cka04-str
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Mi
```

```bash
kubectl apply -f <template-file-name>.yaml
```

---

## Q16. Fix Broken Cluster Due to Missing Kubelet

**SECTION: TROUBLESHOOTING**

**Cluster**: `cluster2-controlplane`

### Solution:

```bash
ssh cluster2-controlplane
kubectl get nodes
# ERROR: The connection to the server ... was refused
crictl ps -a
systemctl status kubelet
# ERROR: kubelet not found
kubeadm version  # e.g. v1.32.0
sudo apt install -y kubelet=1.32.0-1.1
sudo systemctl start kubelet
kubectl get nodes
```

---

Q. 7

Task
SECTION: TROUBLESHOOTING

Solve this question on: ssh cluster4-controlplane

A pod called pink-pod-cka16-trb is created in the default namespace in cluster4. This app runs on port tcp/5000, and it is to be exposed to end-users using an ingress resource called pink-ing-cka16-trb such that it becomes accessible using the command curl http://kodekloud-pink.app on the cluster4-controlplane host. There is an ingress.yaml file under the root folder in cluster4-controlplane. Create an ingress resource by following the command and continue with the task.

kubectl create -f ingress.yaml

However, even after creating the ingress resource, it is not working. Troubleshoot and fix this issue, making any necessary changes to the objects.

Solution
SSH into the cluster4-controlplane host
ssh cluster4-controlplane

create ingress with the given yaml file ingress.yaml

kubectl create -f ingress.yaml

Now try to access the app.

curl kodekloud-pink.app

You must be getting 503 Service Temporarily Unavailable error.
Let's look into the service:

kubectl edit svc pink-svc-cka16-trb

Under ports: change protocol: UDP to protocol: TCP
Try to access the app again

curl kodekloud-pink.app

It should work now.

Details

Is the App is accessible?

Task
SECTION: WORKLOADS & SCHEDULING

Solve this question on: ssh cluster3-controlplane

We have deployed a 2-tier web application on the cluster3 nodes in the canara-wl05 namespace. However, at the moment, the web app pod cannot establish a connection with the MySQL pod successfully.

You can check the status of the application from the terminal by running the curl command with the following syntax:

curl http://cluster3-controlplane:NODE-PORT

To make the application work, create a new secret called db-secret-wl05 with the following key values: -

1. DB_Host=mysql-svc-wl05
2. DB_User=root
3. DB_Password=password123

Next, configure the web application pod to load the new environment variables from the newly created secret.

Note: Check the web application again using the curl command, and the status of the application should be success.

Solution
SSH into the cluster3-controlplane host
ssh cluster3-controlplane

List the nodes: -

kubectl get nodes -o wide

Run the curl command to know the status of the application as follows: -

curl http://192.168.222.173:31020

<!doctype html>
<title>Hello from Flask</title>
...

    <img src="/static/img/failed.png">
    <h3> Failed connecting to the MySQL database. </h3>


    <h2> Environment Variables: DB_Host=Not Set; DB_Database=Not Set; DB_User=Not Set; DB_Password=Not Set; 2003: Can&#39;t connect to MySQL server on &#39;localhost:3306&#39; (111 Connection refused) </h2>

As you can see, the status of the application pod is failed.

NOTE: - In your lab, IP addresses could be different.

Let's create a new secret called db-secret-wl05 as follows: -

kubectl create secret generic db-secret-wl05 -n canara-wl05 --from-literal=DB_Host=mysql-svc-wl05 --from-literal=DB_User=root --from-literal=DB_Password=password123

After that, configure the newly created secret to the web application pod as follows: -

---

apiVersion: v1
kind: Pod
metadata:
labels:
run: webapp-pod-wl05
name: webapp-pod-wl05
namespace: canara-wl05
spec:
containers:

- image: kodekloud/simple-webapp-mysql
  name: webapp-pod-wl05
  envFrom:
  - secretRef:
    name: db-secret-wl05

then use the kubectl replace command: -

kubectl replace -f <FILE-NAME> --force

In the end, make use of the curl command to check the status of the application pod. The status of the application should be success.

curl http://192.168.222.173:31020

<!doctype html>
<title>Hello from Flask</title>
<body style="background: #39b54b;"></body>
<div style="color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;">

    <img src="/static/img/success.jpg">
    <h3> Successfully connected to the MySQL database.</h3>

Details

Task
SECTION: TROUBLESHOOTING

Solve this question on: ssh cluster4-controlplane

A pod called pink-pod-cka16-trb is created in the default namespace in cluster4. This app runs on port tcp/5000, and it is to be exposed to end-users using an ingress resource called pink-ing-cka16-trb such that it becomes accessible using the command curl http://kodekloud-pink.app on the cluster4-controlplane host. There is an ingress.yaml file under the root folder in cluster4-controlplane. Create an ingress resource by following the command and continue with the task.

kubectl create -f ingress.yaml

However, even after creating the ingress resource, it is not working. Troubleshoot and fix this issue, making any necessary changes to the objects.

Solution
SSH into the cluster4-controlplane host
ssh cluster4-controlplane

create ingress with the given yaml file ingress.yaml

kubectl create -f ingress.yaml

Now try to access the app.

curl kodekloud-pink.app

You must be getting 503 Service Temporarily Unavailable error.
Let's look into the service:

kubectl edit svc pink-svc-cka16-trb

Under ports: change protocol: UDP to protocol: TCP
Try to access the app again

curl kodekloud-pink.app

It should work now.

Details

Is the App is accessible?

####

Task
SECTION: TROUBLESHOOTING

Solve this question on: ssh cluster4-controlplane

A pod called pink-pod-cka16-trb is created in the default namespace in cluster4. This app runs on port tcp/5000, and it is to be exposed to end-users using an ingress resource called pink-ing-cka16-trb such that it becomes accessible using the command curl http://kodekloud-pink.app on the cluster4-controlplane host. There is an ingress.yaml file under the root folder in cluster4-controlplane. Create an ingress resource by following the command and continue with the task.

kubectl create -f ingress.yaml

However, even after creating the ingress resource, it is not working. Troubleshoot and fix this issue, making any necessary changes to the objects.

Solution
SSH into the cluster4-controlplane host
ssh cluster4-controlplane

create ingress with the given yaml file ingress.yaml

kubectl create -f ingress.yaml

Now try to access the app.

curl kodekloud-pink.app

You must be getting 503 Service Temporarily Unavailable error.
Let's look into the service:

kubectl edit svc pink-svc-cka16-trb

Under ports: change protocol: UDP to protocol: TCP
Try to access the app again

curl kodekloud-pink.app

It should work now.

###

Solve this question on: ssh cluster1-controlplane

A deployment called nodeapp-dp-cka08-trb is created in the default namespace on cluster1. This app is using an ingress resource named nodeapp-ing-cka08-trb.

From cluster1-controlplane host, we should be able to access this app using the command curl http://kodekloud-ingress.app. However, it is not working at the moment. Troubleshoot and fix the issue.

Solution
SSH into the cluster1-controlplane host
ssh cluster1-controlplane

Try to access the app using curl http://kodekloud-ingress.app command. You will see 404 Not Found error.

Look into the ingress to make sure its configued properly.

kubectl get ingress
kubectl edit ingress nodeapp-ing-cka08-trb

Under rules: -> host: change example.com to kodekloud-ingress.app
Under backend: -> service: -> name: Change example-service to nodeapp-svc-cka08-trb
Change port: -> number: from 80 to 3000
You should be able to access the app using curl http://kodekloud-ingress.app command now.

###

Solve this question on: ssh cluster3-controlplane

Modify the existing web-gateway on cka5673 namespace to handle HTTPS traffic on port 443 for kodekloud.com, using a TLS certificate stored in a secret named kodekloud-tls.

Solution
SSH into the cluster3-controlplane host
ssh cluster3-controlplane

Check the configuration of the web-gateway.

cluster3-controlplane ~ ➜ kubectl get gateway web-gateway -n cka5673 -o yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: web-gateway
namespace: cka5673
resourceVersion: "21919"
uid: a1d0e35d-5126-4000-88ec-f440941eed75
spec:
gatewayClassName: kodekloud
listeners:

- allowedRoutes:
  namespaces:
  from: Same
  name: https
  port: 80
  protocol: HTTP

The current configuration of the web-gateway is incorrect as it is listening on port 80 using the HTTP protocol. To update the web-gateway to listen on port 443 with the TLS certificate, use the following manifest:

apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: web-gateway
namespace: cka5673
spec:
gatewayClassName: kodekloud
listeners: - name: https
protocol: HTTPS
port: 443
hostname: kodekloud.com
tls:
certificateRefs: - name: kodekloud-tls

###

olve this question on: ssh cluster2-controlplane

As a Kubernetes administrator, you are unable to run any of the kubectl commands on the cluster. Troubleshoot the problem and get the cluster to a functioning state.

Solution
SSH into the Control Plane
Run the following command to access cluster2-controlplane:

ssh cluster2-controlplane

1. Check Cluster Status
   Run kubectl get nodes to check the cluster status:

cluster2-controlplane ~ ➜ kubectl get nodes
The connection to the server cluster2-controlplane:6443 was refused - did you specify the right host or port?

This indicates that the API server is down.

2. Verify Kubernetes Containers Are Running
   Check if all the Kubernetes-related containers are running:

cluster2-controlplane ~ ➜ crictl ps -a

CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID POD
fbbf1db37defb ead0a4a53df89 5 hours ago Running coredns 0 4be392af8ae56 coredns-69f9c977-lsjwc
b7a2fc42b8b83 ead0a4a53df89 5 hours ago Running coredns 0 8fdd89e10d6d7 coredns-69f9c977-bfsfw
63f27f8d81c29 a0eed15eed449 5 hours ago Running etcd 0 3e093d91c4361 etcd-cluster2-controlplane
f07cb1acaa26f 0824682bcdc8e 5 hours ago Exited kube-controller-manager 0 fba0d299aab89 kube-controller-manager-cluster2-controlplane
2f36c95ff0b7e 7ace497ddb8e8 5 hours ago Exited kube-scheduler 0 11e422ba2d28b kube-scheduler-cluster2-controlplane

The kube-apiserver container is missing.

3. Check kubelet Logs
   Since kube-apiserver is missing, check if kubelet is running:

cluster2-controlplane ~ ➜ systemctl status kubelet

Unit kubelet.service could not be found.

This confirms that kubelet is missing or not installed.

4. Reinstall kubelet
   Since kubelet is missing, we need to install it manually.

Find the Correct Kubelet Version
cluster2-controlplane ~ ➜ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"32", GitVersion:"v1.32.0", GitCommit:"70d3cc986aa8221cd1dfb1121852688902d3bf53", GitTreeState:"clean", BuildDate:"2024-12-11T18:04:20Z", GoVersion:"go1.23.3", Compiler:"gc", Platform:"linux/amd64"}

The kubelet version must match kubeadm.

Install kubelet (Matching Version)
sudo apt install -y kubelet=1.32.0-1.1

Start the kubelet Service\*\*
sudo systemctl start kubelet

5. Verify That the Cluster is Restored
   Wait a few moments for kubelet to bring back kube-apiserver, then check the cluster status:

cluster2-controlplane ~ ➜ kubectl get nodes

NAME STATUS ROLES AGE VERSION
cluster2-controlplane Ready control-plane 27m v1.32.0
cluster2-node01 Ready <none> 26m v1.32.0

Details
