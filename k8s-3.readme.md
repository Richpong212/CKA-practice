# Kubernetes Exam README - Solution Guide

---

## ✅ Q1: Fix Pending Pod with Wrong Resource Unit

**Cluster:** `cluster3-controlplane`

**Task:**

- Pod `nginx-wl06` is stuck in Pending due to incorrect unit (Gi instead of Mi).
- Delete and recreate with `100Mi`.

**Solution:**

```bash
kubectl delete pod nginx-wl06
# Fix YAML to use 100Mi
kubectl apply -f nginx-wl06-fixed.yaml
```

---

## ✅ Q2: Remove Helm Chart with Vulnerable Image

**Cluster:** `cluster1-controlplane`

**Task:**

> On the cluster1, the team has installed multiple Helm charts in different namespaces.
> By mistake, one of the deployed resources includes a **vulnerable image** called:
>
> ```
> kodekloud/click-counter:latest
> ```
>
> Find out **which Helm release** is using this image and **uninstall it**.

**Solution:**

1. **Search for the vulnerable image:**

```bash
kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.namespace}{{"\t"}}{.metadata.name}{{"\t"}}{range .spec.containers[*]}{.image}{"\n"}{end}{end}' | grep kodekloud/click-counter
```

2. **Identify the Helm release:**

```bash
helm list -A
```

3. **Uninstall the release:**

```bash
helm uninstall web-dashboard-apd -n web-dashboard-03
```

4. **Verify the release is gone:**

```bash
helm list -A
```

---

## ✅ Q3: Create ClusterIP Service & Save Pod IPs

**Cluster:** `cluster3-controlplane`

**Part I:** Create service `service-3421-svcn` in `spectra-1267`

```bash
kubectl get pods -n spectra-1267 --show-labels
# Confirm pods: pod-21 & pod-23 → labels: mode=exam,type=external
kubectl create service clusterip service-3421-svcn -n spectra-1267 --tcp=8080:80 --dry-run=client -o yaml > service.yaml
# Edit selectors to mode=exam,type=external
kubectl apply -f service.yaml
```

**Part II:** Store sorted pod IPs

```bash
kubectl get pods -n spectra-1267 -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP > /root/pod_ips_cka05_svcn
```

---

## ✅ Q4: List VPA CRDs

**Cluster:** `cluster2-controlplane`

```bash
echo "verticalpodautoscalercheckpoints.autoscaling.k8s.io" > /root/vpa-crds.txt
echo "verticalpodautoscalers.autoscaling.k8s.io" >> /root/vpa-crds.txt
```

---

## ✅ Q5: HPA for Deployment

**Cluster:** `cluster1-controlplane`

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-ui-hpa
  namespace: ck1967
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-ui-deployment
  minReplicas: 2
  maxReplicas: 12
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 65
  behavior:
    scaleUp:
      policies:
        - type: Percent
          value: 20
          periodSeconds: 45
    scaleDown:
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
```

---

## ⏳ Q6: Troubleshoot Crashing Pod

**Cluster:** `cluster1-controlplane`

- Investigate `green-deployment-cka15-trb` crashing pods.
- Use `kubectl logs` and `kubectl describe pod` to fix.

---

## ✅ Q7: Create VPA for StatefulSet

**Cluster:** `cluster2-controlplane`

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: cache-vpa
  namespace: caching
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: cache-statefulset
  updatePolicy:
    updateMode: Initial
```

---

## ⏳ Q8: Add PVC to Pod

**Cluster:** `cluster1-controlplane`

- Update `/root/peach-pod-cka05-str.yaml` to:

  - Add `peach-pvc-cka05-str` claiming 100Mi from `peach-pv-cka05-str`
  - Mount to `/var/www/html`

---

## ⏳ Q9: HTTPRoute Traffic Split

**Cluster:** `cluster2-controlplane`

- Create `HTTPRoute` in `cka3658` namespace.
- Route 80% to `web-portal-service-v1` and 20% to `web-portal-service-v2`.

---

## ⏳ Q10: Fix Pending Pod

**Cluster:** `cluster1-controlplane`

- Pod `demo-pod-cka29-trb` is Pending
- Run `kubectl describe pod` to determine issue and fix.

---

## ⏳ Q11: Add PVC + Sidecar

**Cluster:** `cluster1-controlplane`

- YAML file: `/root/olive-app-cka10-str.yaml`
- Add:

  - PVC: `olive-pvc-cka10-str`
  - Sidecar `busybox` with sleep, read-only mount at `/usr/src`

---

## ⏳ Q12: Stop Scheduler Temporarily

**Cluster:** `cluster2-controlplane`

- Stop scheduler
- Create `onyx-pod` (runs on controlplane)
- Restart scheduler
- Create `ember-pod` (runs on node01)

---

## ⏳ Q13: KCM Restart Issue

**Cluster:** `cluster4-controlplane`

- Investigate kube-controller-manager crashes
- Check logs, systemd units, certificates, etc.

---

## ⏳ Q14: External Web Server Service Not Working

**Cluster:** `cluster3-controlplane`

- Fix `external-webserver-cka03-svcn` service to reach external IP (student-node:9999)

---

## ✅ Q15: Create ConfigMap from File

**Cluster:** `cluster1-controlplane`

```bash
kubectl create configmap db-user-pass-cka17-arch --from-file=/opt/db-user-pass
```

---

## ⏳ Q16: NetworkPolicy Too Restrictive

**Cluster:** `cluster1-controlplane`

- Pod: `cyan-pod-cka28-trb` in `cyan-ns-cka28-trb`
- Accessible **only** from `cyan-white-cka28-trb`
- Update `cyan-np-cka28-trb` to allow correct traffic

#### SOLUTIONS

Task 3
SECTION: SERVICES AND NETWORKING

Solve this question on: ssh cluster3-controlplane

Part I:

Create a ClusterIP service with name of service-3421-svcn in the spectra-1267 namespace, which should expose the pods, namely, pod-23 and pod-21, with port set to 8080 and targetport to 80.

Part II:

Store the pod names and their IP addresses from the spectra-1267 namespace at /root/pod_ips_cka05_svcn where the output is sorted by their IPs.

Please ensure the format as shown below:

POD_NAME IP_ADDR
pod-1 ip-1
pod-3 ip-2
pod-2 ip-3
...

Solution
SSH into the cluster3-controlplane host
ssh cluster3-controlplane

The easiest way to route traffic to a specific pod is by the use of labels and selectors. List the pods along with their labels:

cluster3-controlplane ~ ➜ kubectl get pods --show-labels -n spectra-1267
NAME READY STATUS RESTARTS AGE LABELS
pod-12 1/1 Running 0 5m21s env=dev,mode=standard,type=external
pod-34 1/1 Running 0 5m20s env=dev,mode=standard,type=internal
pod-43 1/1 Running 0 5m20s env=prod,mode=exam,type=internal
pod-23 1/1 Running 0 5m21s env=dev,mode=exam,type=external
pod-32 1/1 Running 0 5m20s env=prod,mode=standard,type=internal
pod-21 1/1 Running 0 5m20s env=prod,mode=exam,type=external

Looks like there are a lot of pods created to confuse us. But we are only concerned with the labels of pod-23 and pod-21.

As we can see both the required pods have labels mode=exam,type=external in common. Let's confirm that using kubectl too:

cluster3-controlplane ~ ➜ kubectl get pod -l mode=exam,type=external -n spectra-1267  
NAME READY STATUS RESTARTS AGE
pod-23 1/1 Running 0 9m18s
pod-21 1/1 Running 0 9m17s

Now as we have figured out the labels, we can proceed further with the creation of the service:

kubectl create service clusterip service-3421-svcn -n spectra-1267 --tcp=8080:80 --dry-run=client -o yaml > service-3421-svcn.yaml

Now modify the service definition with selectors as required before applying to k8s cluster:

cluster3-controlplane ~ ➜ cat service-3421-svcn.yaml
apiVersion: v1
kind: Service
metadata:
creationTimestamp: null
labels:
app: service-3421-svcn
name: service-3421-svcn
namespace: spectra-1267
spec:
ports:

- name: 8080-80
  port: 8080
  protocol: TCP
  targetPort: 80
  selector:
  app: service-3421-svcn # delete
  mode: exam # add
  type: external # add
  type: ClusterIP
  status:
  loadBalancer: {}

Finally let's apply the service definition:

kubectl apply -f service-3421-svcn.yaml

You can now validate there are two endpoints to the service

cluster3-controlplane ~ ➜ k get ep service-3421-svcn -n spectra-1267
NAME ENDPOINTS AGE
service-3421 10.42.0.15:80,10.42.0.17:80 52s

To list all the pod name along with their IP's, we could use imperative command as shown below:

kubectl get pods -n spectra-1267 -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP

POD_NAME IP_ADDR
pod-12 10.42.0.18
pod-23 10.42.0.19
pod-34 10.42.0.20
pod-21 10.42.0.21
...

# store the output to /root/pod_ips

kubectl get pods -n spectra-1267 -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP > /root/pod_ips_cka05_svcn

### task 9

Task
SECTION: SERVICES AND NETWORKING

Solve this question on: ssh cluster2-controlplane

Configure a web-portal-httproute within the cka3658 namespace to facilitate traffic distribution. Route 80% of the traffic to web-portal-service-v1 and 20% to the new version, web-portal-service-v2.

Note: Gateway has already been created in the nginx-gateway namespace.
To test the gateway, execute the following command:

curl http://cluster2-controlplane:30080

Solution
SSH into the cluster2-controlplane host
ssh cluster2-controlplane

Next, check all the deployments and services in the cka3658 namespace with the command:

kubectl get deploy,svc -n cka3658

Review the gateway that has been created in the nginx-gateway namespace using the command:

kubectl get gateway -n nginx-gateway

You should see output similar to:

NAME CLASS ADDRESS PROGRAMMED AGE
nginx-gateway nginx True 36m

Now, create an HTTPRoute in the cka3658 namespace with the name web-portal-httproute to distribute traffic based on specified weights. Use the following YAML configuration:

apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: web-portal-httproute
namespace: cka3658
spec:
parentRefs: - name: nginx-gateway
namespace: nginx-gateway
rules: - matches: - path:
type: PathPrefix
value: /
backendRefs: - name: web-portal-service-v1
port: 80
weight: 80 - name: web-portal-service-v2
port: 80
weight: 20

Details

Is the HttpRoute referring to the NGINX gateway?

Is the web-portal-httproute routing 80% traffic to web-portal-service-v1?

Is the web-portal-httproute routing 20% traffic to web-portal-service-v2?

Is the web-portal-httproute distributing traffic?

### task 11

Task
SECTION: STORAGE

Solve this question on: ssh cluster1-controlplane

We want to deploy a Python-based application on the cluster using a template located at /root/olive-app-cka10-str.yaml on cluster1-controlplane. However, before you proceed, we need to make some modifications to the YAML file as per the following details:

The YAML should also contain a persistent volume claim with name olive-pvc-cka10-str to claim a 100Mi of storage from olive-pv-cka10-str PV.

Update the deployment to add a co-located container named busybox, which can use busybox image (you might need to add a sleep command for this container to keep it running.)

Share the python-data volume with this container and mount the same at path /usr/src. Make sure this container only has read permissions on this volume.

Finally, create a pod using this YAML and make sure the POD is in Running state.

Note: Additionally, you can expose a NodePort service for the application. The service should be named olive-svc-cka10-str and expose port 5000 with a nodePort value of 32006.
However, inclusion/exclusion of this service won't affect the validation for this task.

Solution
SSH into the cluster1-controlplane host
ssh cluster1-controlplane

Update olive-app-cka10-str.yaml template so that it looks like as below:

---

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: olive-pvc-cka10-str
spec:
accessModes:

- ReadWriteMany
  storageClassName: olive-stc-cka10-str
  volumeName: olive-pv-cka10-str
  resources:
  requests:
  storage: 100Mi

---

apiVersion: apps/v1
kind: Deployment
metadata:
name: olive-app-cka10-str
spec:
replicas: 1
template:
metadata:
labels:
app: olive-app-cka10-str
spec:
affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname
operator: In
values: - cluster1-node01
containers: - name: python
image: poroko/flask-demo-app
ports: - containerPort: 5000
volumeMounts: - name: python-data
mountPath: /usr/share/ - name: busybox
image: busybox
command: - "bin/sh" - "-c" - "sleep 10000"
volumeMounts: - name: python-data
mountPath: "/usr/src"
readOnly: true
volumes: - name: python-data
persistentVolumeClaim:
claimName: olive-pvc-cka10-str
selector:
matchLabels:
app: olive-app-cka10-str

---

apiVersion: v1
kind: Service
metadata:
name: olive-svc-cka10-str
spec:
type: NodePort
ports: - port: 5000
nodePort: 32006
selector:
app: olive-app-cka10-str

Apply the template:

kubectl apply -f olive-app-cka10-str.yaml

### task 13

Task
SECTION: TROUBLESHOOTING

Solve this question on: ssh cluster4-controlplane

On cluster4, we are seeing an intermittent issue where the following error appears while running kubectl commands.

The connection to the server cluster4-controlplane:6443 was refused - did you specify the right host or port?

Whenever you get this error, you can wait for 10-15 seconds for the kubectl command to work again, but the issue recurs after few seconds.

We also noticed that the kube-controller-manager-cluster4-controlplane pod is restarting continuously. Look into the issue and troubleshoot it.

Note: After updating, the system pods might take a little time to reach the running state, so wait for a few minutes after making changes.

Solution
SSH into the cluster4-controlplane host
ssh cluster4-controlplane

Let's check the POD status
kubectl get pod -n kube-system

You will see that kube-controller-manager-cluster4-controlplane pod is crashing or restarting. So let's try to watch the logs.

kubectl logs -f kube-controller-manager-cluster4-controlplane -n kube-system

You will see some logs as below:

leaderelection.go:330] error retrieving resource lock kube-system/kube-controller-manager: Get "https://10.10.129.21:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s": dial tcp 10.10.129.21:6443: connect: connection refused

You will notice that somehow the connection to the kube api is breaking, let's check if kube api pod is healthy.

kubectl get pod -n kube-system

Now you might notice that kube-apiserver-cluster4-controlplane pod is also restarting, so we should dig into its logs or relevant events.

kubectl logs -f kube-apiserver-cluster4-controlplane -n kube-system
kubectl get event --field-selector involvedObject.name=kube-apiserver-cluster4-controlplane -n kube-system

In events you will see this error

Warning Unhealthy pod/kube-apiserver-cluster4-controlplane Liveness probe failed: Get "https://10.10.132.25:6444/livez": dial tcp 10.10.132.25:6444: connect: connection refused

From this we can see that the Liveness probe is failing for the kube-apiserver-cluster4-controlplane pod, and we can see its trying to connect to port 6444 port but the default api port is 6443. So let's look into the kube api server manifest.

vi /etc/kubernetes/manifests/kube-apiserver.yaml

Under livenessProbe: you will see the port: value is 6444, change it to 6443 and save. Now wait for few seconds let the kube api pod come up.

kubectl get pod -n kube-system

Watch the PODs status for some time and make sure these are not restarting now.

### task 14

Solve this question on: ssh cluster3-controlplane

We have an external webserver running on student-node which is exposed at port 9999. We have created a service called external-webserver-cka03-svcn that can connect to our local webserver from within the kubernetes cluster3, but at the moment, it is not working as expected.

Fix the issue so that other pods within cluster3 can use external-webserver-cka03-svcn service to access the webserver.

Solution
SSH into the cluster3-controlplane host
ssh cluster3-controlplane

Let's check if the webserver is working or not:

cluster3-controlplane ~ ➜ curl student-node:9999
...

<h1>Welcome to nginx!</h1>
...

Now we will check if service is correctly defined:

cluster3-controlplane ~ ➜ kubectl describe svc -n kube-public external-webserver-cka03-svcn
Name: external-webserver-cka03-svcn
Namespace: kube-public
.
.
Endpoints: <none> # there are no endpoints for the service
...

As we can see there is no endpoints specified for the service, hence we won't be able to get any output. Since we can not destroy any k8s object, let's create the endpoint manually for this service as shown below:

First, obtain the IP address of the student node, easiest way is to ping it:

root@student-node ~ ✦ ping student-node
PING student-node (192.168.222.128) 56(84) bytes of data.
64 bytes from student-node (192.168.222.128): icmp_seq=1 ttl=64 time=0.023 ms
64 bytes from student-node (192.168.222.128): icmp_seq=2 ttl=64 time=0.030 ms

In this example : student-node IP is 192.168.222.128

Next, use the IP address to create the EndpointSlice:

cluster3-controlplane ~ ➜ kubectl apply -f - <<EOF
apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
name: external-webserver-cka03-svcn
namespace: kube-public
labels:
kubernetes.io/service-name: external-webserver-cka03-svcn
addressType: IPv4
ports:

- protocol: TCP
  port: 9999
  endpoints:
- addresses: - 192.168.222.128 # IP of student node
  EOF

Finally check if the curl test works now:

cluster3-controlplane ~ ➜ kubectl run -n kube-public --rm -i test-curl-pod --image=curlimages/curl --restart=Never -- curl -m 2 external-webserver-cka03-svcn
...

<title>Welcome to nginx!</title>
...

### task 16

ask
SECTION: TROUBLESHOOTING

Solve this question on: ssh cluster1-controlplane

An nginx-based pod called cyan-pod-cka28-trb is running under the cyan-ns-cka28-trb namespace and is exposed within the cluster using the cyan-svc-cka28-trb service.

This is a restricted pod, so a network policy called cyan-np-cka28-trb has been created in the same namespace to apply some restrictions on this pod.

Two other pods called cyan-white-cka28-trb and cyan-black-cka28-trb are also running in the default namespace.

The nginx-based app running on the cyan-pod-cka28-trb pod is exposed internally on the default nginx port (80).

Expectation: This app should only be accessible from the cyan-white-cka28-trb pod.

Problem: This app is not accessible from anywhere.

Troubleshoot this issue and fix the connectivity as per the requirement listed above.

Note: You can exec into cyan-white-cka28-trb and cyan-black-cka28-trb pods and test connectivity using the curl utility.

You may update the network policy, but make sure it is not deleted from the cyan-ns-cka28-trb namespace.

Solution
SSH into the cluster1-controlplane host
ssh cluster1-controlplane

Let's look into the network policy

kubectl edit networkpolicy cyan-np-cka28-trb -n cyan-ns-cka28-trb

Under spec: -> egress: you will notice there is not cidr: block has been added, since there is no restrcitions on egress traffic so we can update it as below. Further you will notice that the port used in the policy is 8080 but the app is running on default port which is 80 so let's update this as well (under egress and ingress):

Change port: 8080 to port: 80

- ports:
  - port: 80
    protocol: TCP
    to:
  - ipBlock:
    cidr: 0.0.0.0/0

Now, lastly notice that there is no POD selector has been used in ingress section but this app is supposed to be accessible from cyan-white-cka28-trb pod under default namespace. So let's edit it to look like as below:

ingress:

- from:
  - namespaceSelector:
    matchLabels:
    kubernetes.io/metadata.name: default
    podSelector:
    matchLabels:
    app: cyan-white-cka28-trb

Now, let's try to access the app from cyan-white-pod-cka28-trb

kubectl exec cyan-white-cka28-trb -- sh -c 'curl cyan-svc-cka28-trb.cyan-ns-cka28-trb.svc.cluster.local'

Also make sure its not accessible from the other pod(s)

kubectl exec cyan-black-cka28-trb -- sh -c 'curl cyan-svc-cka28-trb.cyan-ns-cka28-trb.svc.cluster.local'

It should not work from this pod. So its looking good now.
