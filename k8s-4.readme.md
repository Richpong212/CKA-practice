## ðŸ§© Question 1

**Solve this question on:** `ssh cluster1-controlplane`

A template to create a Kubernetes Pod is stored at `/root/red-probe-cka12-trb.yaml` on the `cluster1-controlplane`.
However, using this template as is results in an error.

Fix the issue with this template and use it to create the Pod.
Once created, watch the Pod for a minute or two to ensure it is stable (i.e., not crashing or restarting).

> âš ï¸ **Note:** Do **not** update the `args:` section of the template.

---

### âœ… **Answer**

#### 1. SSH into the cluster

```bash
ssh cluster1-controlplane
```

#### 2. Try to apply the template

```bash
kubectl apply -f red-probe-cka12-trb.yaml
```

You will see an error:

```
error: error validating "red-probe-cka12-trb.yaml": error validating data: [ValidationError(Pod.spec.containers[0].livenessProbe.httpGet): unknown field "command" in io.k8s.api.core.v1.HTTPGetAction, ValidationError(Pod.spec.containers[0].livenessProbe.httpGet): missing required field "port" in io.k8s.api.core.v1.HTTPGetAction]; if you choose to ignore these errors, turn validation off with --validate=false
```

---

#### 3. Identify the issue

From the error, we can see that itâ€™s related to the **liveness probe**.

Open the file:

```bash
vi red-probe-cka12-trb.yaml
```

Under `livenessProbe:` youâ€™ll notice it uses `httpGet` even though it runs a `command`.
So, this should be changed to **`exec`** instead of `httpGet`.

---

#### 4. Fix the probe type

Change:

```yaml
livenessProbe:
  httpGet:
    command:
      - cat
      - /healthcheck
```

To:

```yaml
livenessProbe:
  exec:
    command:
      - cat
      - /healthcheck
```

---

#### 5. Apply the fixed template

```bash
kubectl apply -f red-probe-cka12-trb.yaml
```

It should now create successfully.

---

#### 6. Watch the Pod

```bash
kubectl get pod red-probe-cka12-trb -w
```

After a few seconds, youâ€™ll see the Pod **restarting**. Letâ€™s check why.

---

#### 7. Check events

```bash
kubectl get event --field-selector involvedObject.name=red-probe-cka12-trb
```

Youâ€™ll see something like:

```
Warning   Unhealthy   pod/red-probe-cka12-trb   Liveness probe failed: cat: can't open '/healthcheck': No such file or directory
```

---

#### 8. Analyze the cause

Open the file again:

```bash
vi red-probe-cka12-trb.yaml
```

Notice in the command:

```bash
sleep 3 ; touch /healthcheck; sleep 30; sleep 30000
```

The health file `/healthcheck` is created **after 3 seconds**,
but `initialDelaySeconds` is only **1** and `failureThreshold` is **1**,
so the probe fails too early.

---

#### 9. Fix timing issue

Change:

```yaml
initialDelaySeconds: 1
```

To:

```yaml
initialDelaySeconds: 5
```

---

#### 10. Recreate the Pod

```bash
kubectl delete pod red-probe-cka12-trb
kubectl apply -f red-probe-cka12-trb.yaml
```

---

#### 11. Verify stability

```bash
kubectl get pod red-probe-cka12-trb -w
```

After watching for a minute, the Pod remains **Running** and **Ready** without restarts.

---

### ðŸŽ¯ **Result**

- âœ… Template fixed and applied successfully
- âœ… Pod is stable and not restarting
- âœ… `args:` section remains unchanged

---

##################

## ðŸ§© Question 2

**Solve this question on:** `ssh cluster3-controlplane`

Utilize Helm to search for the repository URL of the **Bitnami version** of the **nginx** repository.
Ensure that you save the repository URL in the file located at
`/root/nginx-helm-url.txt` on the `cluster3-controlplane`.

---

### âœ… **Solution**

#### 1. SSH into the Control Plane

Access the control plane node:

```bash
ssh cluster3-controlplane
```

---

#### 2. Search for the Nginx Helm repository

Use the following Helm command to search for nginx charts on **Artifact Hub**:

```bash
helm search hub nginx --list-repo-url | head -n15
```

You will see a list of nginx repositories similar to this:

```
URL                                                     CHART VERSION   APP VERSION   DESCRIPTION                                             REPO URL
https://artifacthub.io/packages/helm/krakazyabr...      1.0.0           1.19.0        Nginx Helm chart for Kubernetes                         https://krakazyabra.github.io/microservices
https://artifacthub.io/packages/helm/jfrog/nginx        15.1.5          1.25.2        NGINX Open Source is a web server that can be a...      https://charts.jfrog.io
...
https://artifacthub.io/packages/helm/bitnami/nginx      19.0.2          1.27.4        NGINX Open Source is a web server that can be a...      https://charts.bitnami.com/bitnami
```

---

#### 3. Capture the Repository URL

From the list, identify the **Bitnami NGINX** chart entry.
The repository URL is:

```
https://charts.bitnami.com/bitnami
```

Store it in the specified file:

```bash
echo "https://charts.bitnami.com/bitnami" > /root/nginx-helm-url.txt
```

---

#### 4. Verify the file

Confirm the URL is stored correctly:

```bash
cat /root/nginx-helm-url.txt
```

Output:

```
https://charts.bitnami.com/bitnami
```

---

### ðŸŽ¯ **Result**

- âœ… Bitnami NGINX repository URL found successfully
- âœ… URL stored in `/root/nginx-helm-url.txt`
- âœ… Task completed following Helm documentation

---

########################

## ðŸ§© SECTION: TROUBLESHOOTING

**Solve this question on:** `ssh cluster4-controlplane`

Identify and fix the issue that occurs while running `kubectl` commands on `cluster4`.

---

### âœ… **Solution**

#### **Step 1: SSH into the Control Plane Node**

Access the control plane node:

```bash
ssh cluster4-controlplane
```

---

#### **Step 2: Inspect the kube-apiserver Pod**

Check the status of the `kube-apiserver` static pod:

```bash
crictl ps -a | grep kube-apiserver
```

Get the logs to identify possible errors:

```bash
crictl logs <container_id>
```

If the container is restarting repeatedly or `kubectl` cannot connect, the logs will show the root cause.

---

#### **Step 3: Observe Connection Errors in Logs**

Youâ€™ll notice repeated connection errors similar to:

```
grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379"}.
Err: connection error: desc = "error reading server preface: read tcp 127.0.0.1:57110->127.0.0.1:2379: read: connection reset by peer"
Err: connection error: desc = "error reading server preface: EOF"
```

This indicates that the **API server is unable to connect to etcd**.

---

#### **Step 4: Inspect the Static Pod Manifest**

Open the static pod manifest file for the API server:

```bash
sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
```

Look for the etcd flag in the containerâ€™s command section.
If you see:

```yaml
- --etcd-server=http://127.0.0.1:2379
```

Thatâ€™s **incorrect** because:

- The flag should be `--etcd-servers` (plural)
- The connection should use **HTTPS**, not HTTP

âœ… **Fix it:**

```yaml
- --etcd-servers=https://127.0.0.1:2379
```

Save and exit.
The kubelet will automatically restart the static pod.

---

#### **Step 5: Validate the CA Certificate Path**

If the API server logs now show:

```
transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate signed by unknown authority
```

That means the **CA certificate path** is wrong.

Open the same manifest again:

```bash
sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
```

Look for:

```yaml
- --etcd-cafile=/etc/kubernetes/pki/ca.crt
```

This is incorrect.
It must point to the **etcd-specific CA certificate**.

âœ… **Fix it:**

```yaml
- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
```

Save and exit.
The API server will restart automatically.

---

#### **Step 6: Verify the Fix**

Check if everything is working properly:

```bash
kubectl get nodes
kubectl get pods -A
```

Ensure the API server pod is running and stable:

```bash
crictl ps | grep kube-apiserver
```

You should no longer see errors, and `kubectl` commands should function normally.

---

### ðŸŽ¯ **Result**

- âœ… Fixed incorrect flag `--etcd-server` â†’ `--etcd-servers=https://127.0.0.1:2379`
- âœ… Corrected CA file path to `/etc/kubernetes/pki/etcd/ca.crt`
- âœ… kube-apiserver restarted automatically and is now stable
- âœ… `kubectl` commands work successfully on `cluster4`

---

###################
Hereâ€™s the **formatted README section** for this troubleshooting question, matching your previous structure and formatting style:

---

## ðŸ§© SECTION: TROUBLESHOOTING

**Solve this question on:** `ssh cluster1-controlplane`

A deployment called **`nginx-dp-cka04-trb`** has been used to deploy a static website.
The access to this website can be tested by running:

```bash
curl http://cluster1-controlplane:30002
```

However, it is **not working** at the moment.
Troubleshoot and fix the issue.

---

### âœ… **Solution**

#### **Step 1: SSH into the Control Plane Node**

```bash
ssh cluster1-controlplane
```

---

#### **Step 2: List Available Pods**

Check the running pods:

```bash
kubectl get pods
```

Identify the pod associated with the deployment â€” it should look like:

```
nginx-dp-cka04-trb-767b767dc-6c5wk
```

---

#### **Step 3: Check Pod Logs**

Inspect the pod logs:

```bash
kubectl logs -f <pod-name>
```

If no logs are displayed, there may be a mounting or startup issue.

---

#### **Step 4: Check Pod Events**

Retrieve the podâ€™s events for more information:

```bash
kubectl get events --field-selector involvedObject.name=<pod-name>
```

Youâ€™ll see an error similar to:

```
Warning  FailedMount  pod/nginx-dp-cka04-trb-767b767dc-6c5wk  Unable to attach or mount volumes: unmounted volumes=[nginx-config-volume-cka04-trb], unattached volumes=[index-volume-cka04-trb kube-api-access-4fbrb nginx-config-volume-cka04-trb]: timed out waiting for the condition
```

This indicates a **volume mounting issue** â€” specifically, the **nginx-config-volume-cka04-trb** cannot be mounted.

---

#### **Step 5: Inspect the Deployment**

View the deployment configuration:

```bash
kubectl get deploy nginx-dp-cka04-trb -o=yaml
```

Look under the `volumes:` section. Youâ€™ll find:

```yaml
configMap:
  name: nginx-configuration-cka04-trb
```

This config map name seems incorrect â€” letâ€™s verify if it exists.

---

#### **Step 6: Check the ConfigMap**

Try to fetch the config map:

```bash
kubectl get configmap nginx-configuration-cka04-trb
```

Youâ€™ll get an error â€” it doesnâ€™t exist.

List all config maps to find the correct one:

```bash
kubectl get configmap
```

Youâ€™ll see something like:

```
NAME                      DATA   AGE
nginx-config-cka04-trb     1      12m
```

The correct config map name is **`nginx-config-cka04-trb`**, not **`nginx-configuration-cka04-trb`**.

---

#### **Step 7: Edit the Deployment**

Edit the deployment to fix the config map reference:

```bash
kubectl edit deploy nginx-dp-cka04-trb
```

Find the incorrect section:

```yaml
configMap:
  name: nginx-configuration-cka04-trb
```

âœ… **Fix it:**

```yaml
configMap:
  name: nginx-config-cka04-trb
```

Save and exit the editor.
Kubernetes will automatically restart the pods for the updated deployment.

---

#### **Step 8: Verify Pod and Website**

Wait for the new pod to be in **Running** state:

```bash
kubectl get pods
```

Then test the website access:

```bash
curl http://cluster1-controlplane:30002
```

You should now see the expected website content.

#######################

# ðŸ§© SECTION: STORAGE

## Task

**Solve this question on:** `ssh cluster1-controlplane`

There is a requirement to share a volume between two containers that are running within the same pod. Use the following instructions to create the pod and related objects:

- Create a pod named `grape-pod-cka06-str`.
- The main container must be named **nginx** and use the **nginx** image. Mount a volume `grape-vol-cka06-str` at `/var/log/nginx`.
- Add a sidecar container must be named **busybox-sidecar** using the **busybox** image. Keep it running with `sleep 7200`. Mount the same volume `grape-vol-cka06-str` at `/usr/src`.
- The volume should be of type **emptyDir**.

---

## Solution

**SSH into the cluster1-controlplane host**

```bash
ssh cluster1-controlplane
```

**Create a YAML file as below:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: grape-pod-cka06-str
spec:
  restartPolicy: Always
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: grape-vol-cka06-str
          mountPath: /var/log/nginx

    - name: busybox-sidecar
      image: busybox
      command: ["sleep", "7200"]
      volumeMounts:
        - name: grape-vol-cka06-str
          mountPath: /usr/src

  volumes:
    - name: grape-vol-cka06-str
      emptyDir: {}
```

**Apply the template:**

```bash
kubectl apply -f <template-file-name>.yaml
```

####################

# ðŸ§© SECTION: TROUBLESHOOTING Q6

## Task

**Solve this question on:** `ssh cluster1-controlplane`

There is an existing persistent volume called **orange-pv-cka13-trb**.
A persistent volume claim called **orange-pvc-cka13-trb** is created to claim storage from **orange-pv-cka13-trb**.

However, this PVC is stuck in a **Pending** state. As of now, there is no data in the volume.

Troubleshoot and fix this issue, ensuring that **orange-pvc-cka13-trb** PVC is in the **Bound** state.

---

## Solution

**SSH into the cluster1-controlplane host**

```bash
ssh cluster1-controlplane
```

**List the PVC to check its status**

```bash
kubectl get pvc
```

You will see that **orange-pvc-cka13-trb** PVC is in the **Pending** state and is requesting **150Mi** of storage.
Letâ€™s look into the events for more details.

```bash
kubectl get events --sort-by='.metadata.creationTimestamp' -A
```

You will see an error message similar to:

```
Warning   VolumeMismatch   persistentvolumeclaim/orange-pvc-cka13-trb   Cannot bind to requested volume "orange-pv-cka13-trb": requested PV is too small
```

---

**Check the existing Persistent Volume**

```bash
kubectl get pv
```

You will see that **orange-pv-cka13-trb** volume has a **capacity of 100Mi**, which is smaller than the PVC request of **150Mi**.
This is why the PVC cannot bind to the PV.

---

**Edit the PVC configuration**
Export the PVC configuration to a temporary file:

```bash
kubectl get pvc orange-pvc-cka13-trb -o yaml > /tmp/orange-pvc-cka13-trb.yaml
vi /tmp/orange-pvc-cka13-trb.yaml
```

Under:

```yaml
resources:
  requests:
    storage: 150Mi
```

Change it to:

```yaml
resources:
  requests:
    storage: 100Mi
```

Save and exit the file.

---

**Recreate the PVC**
Delete the old PVC and apply the updated configuration:

```bash
kubectl delete pvc orange-pvc-cka13-trb
kubectl apply -f /tmp/orange-pvc-cka13-trb.yaml
```

---

**Verify the Fix**
Check the PVC status again:

```bash
kubectl get pvc
```

You should now see:

```
NAME                  STATUS   VOLUME               CAPACITY   ACCESS MODES   STORAGECLASS   AGE
orange-pvc-cka13-trb  Bound    orange-pv-cka13-trb  100Mi      RWO                           5s
```

---

### ðŸŽ¯ **Result**

- âœ… PVC size adjusted to match PV capacity
- âœ… **orange-pvc-cka13-trb** successfully bound to **orange-pv-cka13-trb**
- âœ… PVC status: **Bound**

---

##############

# ðŸ§© SECTION: WORKLOADS & SCHEDULING

## Task

**Solve this question on:** `ssh cluster2-controlplane`

Identify the **CPU** and **memory** resource capacity on the `cluster2-node01` node and save the results in
`/root/cluster2-node01-cpu.txt` and `/root/cluster2-node01-memory.txt`, respectively, on the `cluster2-controlplane`.

Store the values in the following format:

```
<Resource-name>: <Value>
```

---

## Solution

**SSH into the Control Plane**

To access `cluster2-controlplane`, execute the following command:

```bash
ssh cluster2-controlplane
```

---

### 1. Retrieve Resource Capacity of the Node

To list the resource capacity of the node, run the command below:

```bash
kubectl describe node cluster2-node01
```

You will see output similar to the following:

```
...
Capacity:
  cpu:                16
  ephemeral-storage:  772706776Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             65838280Ki
  pods:               110
...
```

---

### 2. Record the Resource Values

To store the CPU and memory resource values correctly, execute the following commands on `cluster2-controlplane`:

```bash
echo "cpu: 16" > /root/cluster2-node01-cpu.txt
echo "memory: 65838280Ki" > /root/cluster2-node01-memory.txt
```

---

### ðŸŽ¯ **Result**

- âœ… CPU capacity stored in `/root/cluster2-node01-cpu.txt`
- âœ… Memory capacity stored in `/root/cluster2-node01-memory.txt`
- âœ… Task completed successfully

######################

# ðŸ§© SECTION: TROUBLESHOOTING

## Task

**Solve this question on:** `ssh cluster1-controlplane`

A pod called **check-time-cka03-trb** is continuously crashing. Figure out what is causing this and fix it.

Ensure that the **check-time-cka03-trb** POD is in the running state.

This pod prints the current date and time at a pre-defined frequency and saves it to a file. Ensure that it continues this operation once you have fixed it.

---

## Solution

**SSH into the cluster1-controlplane host**

```bash
ssh cluster1-controlplane
```

**Look into the POD logs**

```bash
kubectl logs -f check-time-cka03-trb
```

You may not see any logs so look into the kubernetes events for **check-time-cka03-trb** POD

**Look into the POD events**

```bash
kubectl get event --field-selector involvedObject.name=check-time-cka03-trb
```

You will see an error something like:

```
Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "/bin/bash": stat /bin/bash: no such file or directory: unknown
```

From the error we can see that its not able to execute `/bin/bash` so let's try `/bin/sh`

**Edit the pod**

```bash
kubectl get pod check-time-cka03-trb -o=yaml > check-time-cka03-trb.yaml
```

**Make the required changes in check-time-cka03-trb.yaml template**

```bash
vi check-time-cka03-trb.yaml
```

Under `spec: -> containers: -> command:` change `/bin/bash` to `/bin/sh` and save the file.

**Delete the old pod.**

```bash
kubectl delete pod check-time-cka03-trb
```

**Apply the updated template**

```bash
kubectl apply -f check-time-cka03-trb.yaml
```

###############

# ðŸ§© SECTION: CLUSTER ARCHITECTURE, INSTALLATION & CONFIGURATION

## Task

**Solve this question on:** `ssh cluster1-controlplane`

An etcd backup is already stored at the path **/opt/cluster1_backup_to_restore.db** on the `cluster1-controlplane` node.
Use **/root/default.etcd** as the `--data-dir` and restore it on the `cluster1-controlplane` node itself.

---

## Solution

**SSH into cluster1-controlplane node:**

```bash
student-node ~ âžœ ssh root@cluster1-controlplane
```

---

**Install etcd utility (if not installed already) and restore the backup:**

```bash
cluster1-controlplane ~ âžœ cd /tmp
cluster1-controlplane ~ âžœ export RELEASE=$(curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest | grep tag_name | cut -d '"' -f 4)
cluster1-controlplane ~ âžœ wget https://github.com/etcd-io/etcd/releases/download/${RELEASE}/etcd-${RELEASE}-linux-amd64.tar.gz
cluster1-controlplane ~ âžœ tar xvf etcd-${RELEASE}-linux-amd64.tar.gz ; cd etcd-${RELEASE}-linux-amd64
cluster1-controlplane ~ âžœ mv etcd etcdctl etcdutl /usr/local/bin/
```

---

**Restore the etcd snapshot using the provided backup file:**

```bash
cluster1-controlplane ~ âžœ etcdutl snapshot restore /opt/cluster1_backup_to_restore.db --data-dir=/root/default.etcd
```

---

### ðŸŽ¯ **Result**

- âœ… etcd backup restored successfully using `/opt/cluster1_backup_to_restore.db`
- âœ… Data restored into `/root/default.etcd`
- âœ… Cluster-level recovery complete on `cluster1-controlplane`

---

################### Q9

# ðŸ§© SECTION: WORKLOADS & SCHEDULING

## Task

**Solve this question on:** `ssh cluster3-controlplane`

Create an HPA named `webapp-hpa` for a deployment named `webapp-deployment` in the `default` namespace with service `webapp-service` associated to it.

The HPA should scale the deployment based on CPU utilization, maintaining an average CPU usage of **50%** across all pods with a **minimum of 1** and a **maximum of 3** replicas.

Configure the HPA to **scale down pods cautiously** by setting a stabilization window of **300 seconds** to prevent rapid fluctuations in pod count.

---

## Solution

**SSH into the cluster3-controlplane host**

```bash
ssh cluster3-controlplane
```

**Verify that the webapp-deployment exists:**

```bash
kubectl get deployment webapp-deployment
```

**Create a file named `webapp-hpa.yaml` with the following content:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp-deployment
  minReplicas: 1
  maxReplicas: 3
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
```

**Apply the HPA configuration:**

```bash
kubectl apply -f webapp-hpa.yaml
```

**Verify that the HPA has been created with the correct configuration:**

```bash
kubectl get hpa webapp-hpa
kubectl describe hpa webapp-hpa
```

The output should show the target CPU utilization of **50%** and the scale-down stabilization window of **300 seconds**.

---

## Understanding Stabilization Windows in HPA

Stabilization windows help prevent rapid fluctuations in the number of replicas during scaling events, particularly when metrics fluctuate around the threshold.

When you set a `stabilizationWindowSeconds` value for scaling down:

- The HPA controller keeps track of the desired number of replicas over a period of time (the stabilization window)
- When deciding whether to scale down, it uses the highest recommendation within that window
- This prevents "flapping" where pods would be repeatedly created and deleted in response to brief metric changes

In this case, the **300-second (5-minute)** stabilization window means that the HPA will only scale down if the recommendation to use fewer replicas has been consistent for at least 5 minutes. This helps maintain system stability and prevents unnecessary pod terminations.

You can also set a separate stabilization window for scaling up by configuring the `scaleUp.stabilizationWindowSeconds` parameter. When not specified, Kubernetes uses default values of **300 seconds for scale-down** and **0 seconds for scale-up**.

---

############# Q 10

# ðŸ§© SECTION: WORKLOADS & SCHEDULING

## Task

**Solve this question on:** `ssh cluster3-controlplane`

A manifest file is available at the `/root/app-wl03/` on the cluster3-controlplane node. The file has some issues; we couldn't deploy a pod on the cluster3-controlplane node.

After fixing the issues, deploy the pod, and it should be in a running state.

**NOTE:** - Ensure that the existing limits are unchanged.

---

## Solution

**SSH into the cluster3-controlplane host**

```bash
ssh cluster3-controlplane
```

Use the cd command to move to the given directory: -

```bash
cd /root/app-wl03/
```

While creating the resource, you will see the error output as follows: -

```bash
kubectl create -f app-wl03.yaml
```

```
The Pod "app-wl03" is invalid: spec.containers[0].resources.requests: Invalid value: "1Gi": must be less than or equal to memory limit
```

In the `spec.containers.resources.requests.memory` value is not configured as compare to the memory limit.

As a fix, open the manifest file with the text editor such as vim or nano and set the value to 100Mi or less than 100Mi.

It should be look like as follows: -

```yaml
resources:
  requests:
    memory: 100Mi
  limits:
    memory: 100Mi
```

Final, create the resource from the kubectl create command: -

```bash
kubectl create -f app-wl03.yaml
```

```
pod/app-wl03 created
```

############## Q 12

# ðŸ§© SECTION: SERVICES AND NETWORKING

## Task

**Solve this question on:** `ssh cluster1-controlplane`

John is setting up a two-tier application stack that is supposed to be accessible using the service `curlme-cka01-svcn`. To test that the service is accessible, he is using a pod called `curlpod-cka01-svcn`. However, at the moment, he is unable to get any response from the application.

Troubleshoot and fix this issue to ensure the application stack is accessible.

While you may delete and recreate the service `curlme-cka01-svcn`, please do not alter it in anyway.

---

## Solution

**SSH into the cluster1-controlplane host**

```bash
ssh cluster1-controlplane
```

**Test if the service curlme-cka01-svcn is accessible from pod curlpod-cka01-svcn or not.**

```bash
kubectl exec curlpod-cka01-svcn -- curl curlme-cka01-svcn
```

```
.....
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:00:10 --:--:--     0
```

We did not get any response. Check if the service is properly configured or not.

```bash
kubectl describe svc curlme-cka01-svcn
```

```
....
Name:              curlme-cka01-svcn
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          run=curlme-ckaO1-svcn
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.109.45.180
IPs:               10.109.45.180
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>
```

The service has no endpoints configured. As we can delete the resource, let's delete the service and create the service again.

To delete the service, use the command `kubectl delete svc curlme-cka01-svcn`.
You can create the service using imperative way or declarative way.

**Using imperative command:**

```bash
kubectl expose pod curlme-cka01-svcn --port=80
```

**Using declarative manifest:**

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    run: curlme-cka01-svcn
  name: curlme-cka01-svcn
spec:
  ports:
    - port: 80
      protocol: TCP
      targetPort: 80
  selector:
    run: curlme-cka01-svcn
  type: ClusterIP
```

You can test the connection from `curlpod-cka-1-svcn` using following.

```bash
kubectl exec curlpod-cka01-svcn -- curl curlme-cka01-svcn
```

############### Q13

# ðŸ§© SECTION: STORAGE

## Task

**Solve this question on:** `ssh cluster3-controlplane`

A pod called `elastic-app-cka02-arch` is running in the default namespace. The YAML file for this pod is available at `/root/elastic-app-cka02-arch.yaml` on the cluster3-controlplane. The single application container in this pod writes logs to the file `/var/log/elastic-app.log`.

One of our logging mechanisms needs to read these logs to send them to an upstream logging server, but we don't want to increase the read overhead for our main application container. So, you need to recreate this POD with an additional co-located container named `busybox` that will run along with the application container and print to the STDOUT by running the command `tail -f /var/log/elastic-app.log`. You can use the `busybox` image for this container.

---

## Solution

**SSH into the cluster3-controlplane host**

```bash
ssh cluster3-controlplane
```

**Recreate the pod with a new container called busybox. Update the `/root/elastic-app-cka02-arch.yaml` YAML file as shown below:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: elastic-app-cka02-arch
spec:
  containers:
    - name: elastic-app
      image: busybox:1.28
      args:
        - /bin/sh
        - -c
        - >
          mkdir /var/log; 
          i=0;
          while true;
          do
            echo "$(date) INFO $i" >> /var/log/elastic-app.log;
            i=$((i+1));
            sleep 1;
          done
      volumeMounts:
        - name: varlog
          mountPath: /var/log
    - name: busybox
      image: busybox:1.28
      args: [/bin/sh, -c, "tail -f  /var/log/elastic-app.log"]
      volumeMounts:
        - name: varlog
          mountPath: /var/log
  volumes:
    - name: varlog
      emptyDir: {}
```

**Next, recreate the pod:**

```bash
student-node ~ âžœ kubectl replace -f /root/elastic-app-cka02-arch.yaml --force
pod "elastic-app-cka02-arch" deleted
pod/elastic-app-cka02-arch replaced

student-node ~ âžœ
```

################# Q 14

# ðŸ§© SECTION: WORKLOADS & SCHEDULING

## Task

**Solve this question on:** `ssh cluster3-controlplane`

The CoreDNS configuration in the cluster needs to be updated:

Update the CoreDNS configuration in the cluster so that DNS resolution for `cka.local` works exactly like `cluster.local` and in addition to it.

Test your configuration using the `jrecord/nettools` image by executing the following commands:

```
nslookup kubernetes.default.svc.cluster.local
nslookup kubernetes.default.svc.cka.local
```

---

## Solution

**SSH into the cluster3-controlplane host**

```bash
ssh cluster3-controlplane
```

### 1. Update the CoreDNS Configuration

The CoreDNS configuration is stored in a ConfigMap. To edit this configuration, use the following command:

```bash
kubectl edit cm -n kube-system coredns
```

Below is the structure of the CoreDNS configuration:

```yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cka.local cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
```

### 2. Restart the CoreDNS Deployment

To apply the changes, restart the CoreDNS deployment using the command:

```bash
kubectl rollout restart deploy -n kube-system coredns
```

### 3. Verify the Changes

To verify the updates, execute the following commands:

```bash
kubectl run test --rm -it --image=jrecord/nettools --restart=Never -- nslookup kubernetes.default.svc.cluster.local
kubectl run test --rm -it --image=jrecord/nettools --restart=Never -- nslookup kubernetes.default.svc.cka.local
```

#### Q15

# ðŸ§© SECTION: GATEWAY API (SERVICES & NETWORKING)

## Task

**Solve this question on:** `ssh cluster1-controlplane`

We have a deployment named `external-app` and a service associated with it named `external-service` in the `external` namespace already created.

There is also an existing Ingress resource named `external-ingress` in the `external` namespace.

Create **HTTPRoute** to achieve the same behavior as `external-ingress`.

### Part 1

Create an **HTTP Gateway** named `web-gateway` in the `nginx-gateway` namespace, which uses the **nginx gateway class**, and listens on **port 80**, and **allows routes from all namespaces**.

### Part 2

Create an **HTTPRoute** named `external-route` in the `external` namespace that **binds to the web-gateway** in the `nginx-gateway` namespace and **routes traffic to the external-service** in the `external` namespace.

### Part 3

Remove the existing **Ingress** resource from the `external` namespace.
Hit curl on the gateway to test the service.

> This is the node port where gatewayclass is attached to:

```bash
curl localhost:30080
```

---

## Solution

**SSH into the cluster1-controlplane host**

```bash
ssh cluster1-controlplane
```

### Step 1: Use the GatewayClass named nginx and create a Gateway in the nginx-gateway namespace

Create a file named `nginx-gateway.yaml`:

```yaml
# Create the Gateway
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      port: 80
      protocol: HTTP
      allowedRoutes:
        namespaces:
          from: All
```

**Note:** The `allowedRoutes.namespaces.from: All` setting explicitly allows routes from all namespaces to bind to this Gateway, which is required for our cross-namespace routing scenario.

Apply it:

```bash
kubectl apply -f nginx-gateway.yaml
```

---

### Step 2: Create the HTTPRoute in the external namespace

First, review the existing ingress configuration:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
  creationTimestamp: "2025-03-24T05:21:34Z"
  generation: 1
  name: external-ingress
  namespace: external
  resourceVersion: "6592"
  uid: 2ec75a22-fa6d-45b8-90d8-29273a8fbd0c
spec:
  rules:
    - http:
        paths:
          - backend:
              service:
                name: external-service
                port:
                  number: 80
            path: /
            pathType: Prefix
```

The ingress configuration routes all traffic to the `external-service` on port 80.

Now, create a file named `external-route.yaml` with the following content:

```yaml
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: external-route
  namespace: external
spec:
  parentRefs:
    - name: web-gateway
      namespace: nginx-gateway
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /
      backendRefs:
        - name: external-service
          port: 80
```

Then, apply the HTTPRoute configuration:

```bash
kubectl apply -f external-route.yaml
```

---

### Step 3: Remove the existing ingress resource

To delete the existing ingress resource, execute the following command:

```bash
kubectl delete -n external ingress external-ingress
```

---

### Step 4: Verify that the HTTPRoute is created and configured correctly

Describe the HTTPRoute to see its configuration:

```bash
kubectl describe httproute external-route -n external
```

---

### Step 5: Test the routing functionality

Get the external IP of the Gateway:

```bash
kubectl get gateway web-gateway -n nginx-gateway
```

Test accessing the service using curl:

```bash
curl localhost:30080
```

You should see traffic being routed to the `external-service` in the `external` namespace.

############ Q 16

# ðŸ§© SECTION: SERVICES AND NETWORKING

## Task

**Solve this question on:** `ssh cluster2-controlplane`

Utilize the official flannel definition file, located at `https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml`, to deploy the flannel CNI on the cluster using the CIDR `172.17.0.0/16`. Ensure pods can communicate after the CNI is installed.

---

## Solution

**SSH into the cluster2-controlplane host**

```bash
ssh cluster2-controlplane
```

### 1. Install the Flannel CNI

**Download the Flannel manifest file:**

```bash
wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

**Edit the Flannel manifest to set the CIDR to 172.17.0.0/16:**

```bash
vi kube-flannel.yml
```

**Below is the structure of the CoreDNS configuration:**

```yaml
apiVersion: v1
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "172.17.0.0/16",   #Edited
      "EnableNFTables": false,
      "Backend": {
        "Type": "vxlan"
      }
    }
kind: ConfigMap
metadata:
  labels:
    app: flannel
    k8s-app: flannel
    tier: node
  name: kube-flannel-cfg
  namespace: kube-flannel
```

**Apply the manifest by running the following command:**

```bash
kubectl apply -f kube-flannel.yml
```

### 2. Test the pod-to-pod communication.

**Run an nginx image:**

```bash
kubectl run web-app --image nginx
```

**Retrieve the IP address of the pod:**

```bash
kubectl get pod web-app -o jsonpath='{.status.podIP}'
```

**Test the connection:**

```bash
kubectl run test --rm -it -n kube-public --image=jrecord/nettools --restart=Never -- curl <IP>
```

############# Q17

# ðŸ§© SECTION: CLUSTER ARCHITECTURE, INSTALLATION & CONFIGURATION

## Task

**Solve this question on:** `ssh cluster5-controlplane`

A debian package for **cri-dockerd** `cri-dockerd_0.3.16.3-0.ubuntu-jammy_amd64.deb` is located in the `/root` folder on the `cluster5-controlplane`. As part of cluster initialization, install the package and make sure that **cri-docker service** is **up and enabled** on the system. Additionally, **enable IP forwarding** on the server.

---

## Solution

**SSH into the Control Plane**
To access `cluster5-controlplane`, execute the following command:

```bash
ssh cluster5-controlplane
```

### 1. Install cri-dockerd

The Debian package for cri-dockerd is available at `cri-dockerd_0.3.16.3-0.ubuntu-jammy_amd64.deb`. Install it using the following command:

```bash
dpkg -i cri-dockerd_0.3.16.3-0.ubuntu-jammy_amd64.deb
```

### 2. Enable and Start the Service

To enable and start the cri-dockerd service, use the following command:

```bash
sudo systemctl enable --now cri-docker.service
```

### 3. Enable IP Forwarding

To ensure that IP forwarding changes are persistent:

Create a configuration file:

```bash
vi /etc/sysctl.d/k8s.conf
```

Add the following line to the file:

```
net.ipv4.ip_forward=1
```

Apply the changes:

```bash
sysctl -p
```

To set the changes temporarily, execute the following command:

```bash
sysctl -w net.ipv4.ip_forward=1
```
